{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent :\n",
    "There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent\n",
    "- Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch gradient descent\n",
    "\n",
    "Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \n",
    "θ for the entire training dataset:\n",
    "\n",
    "$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)$\n",
    "\n",
    "As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model online, i.e. with new examples on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example \n",
    "$x^{(i)}$ and label $y^{(i)}$ \n",
    "\n",
    "$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$\n",
    "\n",
    "Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \n",
    "n  training examples:\n",
    "    \n",
    "$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})$\n",
    "\n",
    "This way, it \n",
    "- Reduces the variance of the parameter updates, which can lead to more stable convergence; and \n",
    "- Can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. \n",
    "- Common mini-batch sizes range between 50 and 256, but can vary for different applications. \n",
    "- Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have a dataset of size N  .\n",
    "1. In Batch GD we consider all the data point (all N data points)\n",
    "2. In SGD we consider only 1 datapoint per epoch \n",
    "3. In Mini-Batch` SGD we consider k datapoints per epoch ( k < N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Momentum\n",
    "2. Nesterov accelerated gradient\n",
    "3. Adagrad\n",
    "4. Adadelta\n",
    "5. RMSprop\n",
    "6. Adam\n",
    "7. Nadam\n",
    "8. AMSGrad\n",
    "9. Weigth Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SGD with Momentum : Sochastic Gradient Descent\n",
    "\n",
    "- It takes two steps one 0.9 times the history and 0.1 times the current gradient\n",
    "\n",
    "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another , which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image 2.\n",
    "\n",
    "Momentum is 0.1 of the derivative and 0.9 of it is just the same direction I went last time. \n",
    "\n",
    "Challenges : Overshots its objects and then had to take a U-turn.\n",
    "\n",
    "Image 2: without Momentum\n",
    "![Image 2: SGD without momentum](without_momentum.gif)\n",
    "Image 3 : with Momentum\n",
    "![Image 2: SGD with momentum](with_momentum.gif)\n",
    "\n",
    "Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Image 3. It does this by adding a fraction γ  of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta)$ \n",
    "\n",
    "$\\theta = \\theta - v_t $\n",
    "\n",
    "Note: Some implementations exchange the signs in the equations. The momentum term γ  is usually set to 0.9 or a similar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.youtube.com/embed/6520\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1ba1db71208>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "def display_yotube_video(url, **kwargs):\n",
    "    \"\"\"\n",
    "    Displays a Youtube video in a Jupyter notebook.\n",
    "    \n",
    "    Args:\n",
    "        url (string): a link to a Youtube video.\n",
    "        **kwargs: further arguments for IPython.display.YouTubeVideo\n",
    "    \n",
    "    Returns:\n",
    "        YouTubeVideo: a video that is displayed in your notebook.\n",
    "    \"\"\"\n",
    "    id_ = url.split(\"=\")[-1]\n",
    "    return YouTubeVideo(id_, **kwargs)\n",
    "\n",
    "display_yotube_video(\"https://youtu.be/uQtTwhpv7Ew?t=6520\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum\n",
    "Look before you leap , look ahead gradient \n",
    "\n",
    "- It takes two steps one 0.9 times the history and 0.1 times the gradient step state.\n",
    "- It takes shorter U-turns\n",
    "- NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector).\n",
    "\n",
    "![Image](nesterov.jpeg)\n",
    "\n",
    "- https://www.youtube.com/watch?v=sV9aiEsXanE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad and RMSPROP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Adagrad we use different learning rates for every parameter at time step t.\n",
    "- learning rate = η\n",
    "- Backpropogation :\n",
    "    $\\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i}$\n",
    "- $\\epsilon$ is a smoothing term that avoids division by zero (usually on the order of 1e-8) \n",
    "- $G_{t}$ contains the sum of the squares of the past gradients w.r.t. to all parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "- Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. \n",
    "- Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w\n",
    "- prevent this $G_{t}$ to become very big , as this reduces the learning rate .\n",
    "- $G_{t}$ in Adadelta is weighted moving average and not sum of gradeint sqaure like adagrad.\n",
    "- RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD and SGD+Momentum\n",
    "\n",
    "## Momentum:\n",
    "Accelerate SGD in the relevant direction and dampen side-to-side oscillations\n",
    "It does this by combining a decaying average of previous gradients with the current gradient\n",
    "\n",
    "\n",
    "![Image](SGD_Momentum.png)\n",
    "\n",
    "<span style='background:black; color:white'> SGD  </span>\n",
    "\n",
    "<span style='background:blue;color:white'> SGD+Momentum  </span>\n",
    "\n",
    "![Image](SGD_Momentum.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum and ADAGRAD\n",
    "\n",
    "## ADAGRAD:\n",
    "The idea is that you keep the running sum of squared gradients during optimization. In this case, we have no momentum term, but an expression g that is the sum of the squared gradients.\n",
    "\n",
    "![Image](Momentum_ADAGRAD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD and RMSPROP\n",
    "\n",
    "There is a slight variation of AdaGrad called RMSProp that addresses the problem that AdaGrad has. With RMSProp we still keep the running sum of squared gradients but instead of letting that sum grow continuously over the period of training we let that sum actually decay.\n",
    "In RMSProp we multiply the sum of squared gradients by a decay rate α and add the current gradient weighted by (1- α). The update step in the case of RMSProp looks exactly the same as in AdaGrad where we divide the current gradient by the sum of squared gradients to have this nice property of accelerating the movement along the one dimension and slowing down the movement along the other dimension.\n",
    "\n",
    "\n",
    "![Image](ADAGRAD_RMSPROP.png)\n",
    "![Image](ADAGRAD_RMSPROP_1.png)\n",
    "\n",
    "<span style='background:black; color:white'> SGD  </span>\n",
    "\n",
    "<span style='background:blue;color:white'> SGD+Momentum  </span>\n",
    "\n",
    "<span style='background:red;color:white'> RMSPROP </span>\n",
    "\n",
    "![Image](SGD_MOMS_RMSPROP.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Adam Optimizer\n",
    "\n",
    "The main part of the algorithm consists of the following three equations.\n",
    "\n",
    "![image](Adam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first equation looks a bit like the SGD with momentum. In the case, the term m would be the velocity and β1 the friction term. In the case of Adam, we call m the first momentum and β1 is just a hyperparameter.\n",
    "\n",
    "The difference to SGD with momentum, however, is the factor (1- β1), which is multiplied with the current gradient.\n",
    "\n",
    "The second part of the equations, on the other hand, can be regarded as RMSProp, in which we are keeping the running sum of squared gradients. Also, in this case, there is the factor (1-β2) which is multiplied with the squared gradient.\n",
    "\n",
    "The term v in the equation is called as the second momentum, and β2 is also just a hyperparameter. The final update equation can be seen as a combination of RMSProp and SGD with Momentum.\n",
    "\n",
    "So far, Adam has integrated the nice features of the two previous optimization algorithms, but here’s a little problem, and that’s the question of what happens in the beginning.\n",
    "\n",
    "At the very first time step, the first and second momentum terms are initialized to zero. After the first update of the second momentum, this term is still very close to zero. When we update the weight parameters in the last equation, we divide by a very small second momentum term v, resulting in a very large first update step.\n",
    "\n",
    "This first very large update step is not the result of the geometry of the problem, but it is an artifact of the fact that we have initialized the first and second momentum to zero. To solve the problems of large first update steps, Adam includes a correction clause:\n",
    "\n",
    "![Image](Adam_1.png)\n",
    "\n",
    "You can see that after the first update of the first and second momentum m and v we make an unbiased estimate of these momentums by taking into account the current time step t. These correction terms make the values of the first and second momentum to be higher in the beginning than in the case without the bias correction.\n",
    "\n",
    "As a result, the first update step of the neural network parameters does not get that large and we don’t mess up our training in the beginning. The additional bias corrections give us the full form of Adam Optimizer.\n",
    "\n",
    "<span style='background:black; color:white'> SGD  </span>\n",
    "\n",
    "<span style='background:blue;color:white'> SGD+Momentum  </span>\n",
    "\n",
    "<span style='background:red;color:white'> RMSPROP </span>\n",
    "\n",
    "<span style='background:purple;color:white'> ADAM </span>\n",
    "\n",
    "![Image](Adam.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.deeplearning-academy.com/p/ai-wiki-optimization-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, Adam can be viewed as a combination of RMSprop and momentum: \n",
    "- RMSprop contributes the exponentially decaying average of past squared gradients vt,\n",
    "- while momentum accounts for the exponentially decaying average of past gradients mt. \n",
    "- We have also seen that Nesterov accelerated gradient (NAG) is superior to vanilla momentum.\n",
    "- Nadam (Nesterov-accelerated Adaptive Moment Estimation) thus combines Adam and NAG. In order to incorporate NAG into Adam, we need to modify its momentum term mt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD\n",
    "\n",
    "In settings where Adam converges to a suboptimal solution, it has been observed that some minibatches provide large and informative gradients, but as these minibatches only occur rarely, exponential averaging diminishes their influence, which leads to poor convergence. The authors provide an example for a simple convex optimization problem where the same behaviour can be observed for Adam.\n",
    "\n",
    "To fix this behaviour, the authors propose a new algorithm, AMSGrad that uses the maximum of past squared gradients \n",
    "vt rather than the exponential average to update the parameters. vt is defined the same as in Adam above:\n",
    "\n",
    "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n",
    "\n",
    "Instead of using vt  (or its bias-corrected version ^vt) directly, we now employ the previous \n",
    "vt−1 if it is larger than the current one:\n",
    "\n",
    "$\\hat{v}_t = \\text{max}(\\hat{v}_{t-1}, v_t)$\n",
    "\n",
    "This way, **AMSGrad results in a non-increasing step size**, which avoids the problems suffered by Adam. For simplicity, the authors also remove the debiasing step that we have seen in Adam. The full AMSGrad update without bias-corrected estimates can be seen below:\n",
    "\n",
    "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t  $\n",
    "\n",
    "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $\n",
    "\n",
    "$\\hat{v}_t = \\text{max}(\\hat{v}_{t-1}, v_t)  $\n",
    "\n",
    "$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} m_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional strategies for optimizing SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and Curriculum Learning\n",
    "\n",
    "Generally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch.\n",
    "\n",
    "On the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning .\n",
    "\n",
    "Zaremba and Sutskever were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which sorts examples by increasing difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Another good standard approach to reducing overfitting is Batch Normalization.\n",
    "\n",
    "In general, the inputs to your neural network should always be normalized. Normalization is a process where given a set of data, you subtract from each element the mean value for that data set and divide it by the data set's standard deviation. By doing so, we put the input values onto the same \"scale\", in that all the values have been normalized into units of \"# of standard deviations from the mean\".\n",
    "\n",
    "The reason we would want to put our inputs onto the same scale is because unbalanced inputs with a large range of magnitudes can typically cause instability in neural networks. An extremely large input can often cascade down through the layers. Typically such an imbalance creates gradients that are also wildly imbalanced, and this makes the optimization process difficult to prevent things like explosion. It also creates imbalanced weights. Normalizing inputs avoids this problem.\n",
    "\n",
    "Often times with images, we don't worry about dividing by the standard deviation, but just subtract the mean.\n",
    "\n",
    "Occasionally, these instabilities can arise during training. Imagine that at some point during training, we end up with one extremely large weight. This extremely large weight will then produce an extremely large output value for some element of the output vector, and this imbalance will again pass through the neural network and make it unstable.\n",
    "\n",
    "One idea is to normalize the activation outputs. Unfortunately, this won't prevent SGD from trying to create an imbalanced weight again during the next back-propagation, and trying to solve the problem this way will just cause SGD to continuously try to undo this activation layer normalization. \n",
    "\n",
    "Batch normalize extends this idea with two additions:\n",
    "\n",
    "- after normalizing the activation layer, let's multiply the outputs by an arbitrarily set parameter, and add to that value an additional parameter, therefore setting a new standard deviation and mean\n",
    "\n",
    "- Make all four of these values (the mean and standard deviation for normalization, and the two new parameters to set arbitrary mean and standard deviations after the normalization) trainable.\n",
    "\n",
    "This ensures that the weights don't tend to push very high or very low (since the normalization is included in the gradient calculations, so the updates are aware of the normalization). But it also ensures that if a layer does need to change the overall mean or standard deviation in order to match the output scale, it can do so.\n",
    "\n",
    "By default, you should always include batch normalization, and all modern neural networks do so because:\n",
    "\n",
    "- Adding batchnorm to a model can result in 10x or more improvements in training speed\n",
    "- Because normalization greatly reduces the ability of a small number of outlying inputs to over-influence the training, it also tends to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "You'll notice in observing our metrics while training a Vgg model for Cats and Dogs that our training accuracy is usually lower than our validation. This points to underfitting, and the source of this in Vgg is relatively simple.\n",
    "\n",
    "If you observe the Vgg layers in Keras, you'll notice a layer called Dropout. Dropout (which only happens during training) occurs after the activation layers, and randomly sets activations to zero. Why would we want to randomly delete parts of neural network? It turns out that this allows us to prevent overfitting. We can't overfit exactly to our training data when we're consistently throwing away information learned along the way. This allows our neural network to learn to generalize.\n",
    "\n",
    "In the case of Vgg, the Dropout rate is set to 0.5. This seems quite large, and given the complexity of classification for Imagenet, it seems reasonable to want to make our dropout rate this high. However, for something much simpler like Cats and Dogs, we can see that we're underfitting, and we might have more success by retaining more information and lowering our dropout rate, and retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient noise\n",
    "\n",
    "Neelakantan add noise that follows a Gaussian distribution $N(0, \\sigma^2_t)$ to each gradient update:\n",
    "\n",
    "$g_{t, i} = g_{t, i} + N(0, \\sigma^2_t)$\n",
    "\n",
    "They anneal the variance according to the following schedule:\n",
    "    \n",
    "$\\sigma^2_t = \\dfrac{\\eta}{(1 + t)^\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weigth Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Weigth Decay ? And why do we care ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do a lot of feature engineering and one thing which we do very regularly is to reduce the paramters. \n",
    "- So what is so wrong in this approach ? So why do we care? Why would I want to use more parameters? \n",
    "- Because more parameters means more nonlinearities, more interactions, more curvy bits. \n",
    "- Real life is full of curvy bits. Real life does not look like a straight line. But we don't want them to be more curvy than necessary or more interacting than necessary. Therefore let's use lots of parameters and then penalize complexity.\n",
    "- So one way to penalize complexity is let's sum up the value of your parameters.Now that doesn't quite work because some parameters are positive and some are negative. So what if we sum up the square of the parameters, and that's actually a really good idea.\n",
    "- Let's create a model, and in the loss function we're going to add the sum of the square of the parameters.\n",
    "- Let's actually create a model, and in the loss function we're going to add the sum of the square of the parameters. Now here's a problem with that though. Maybe that number is way too big, and it's so big that the best loss is to set all of the parameters to zero. That would be no good.\n",
    "- So we want to make sure that doesn't happen, so therefore let's not just add the sum of the squares of the parameters to the model but let's multiply that by some number that we choose.That number that we choose is called wd (weigth decay).\n",
    "- loss function and we're going to add to it the sum of the squares of parameters multiplied by some number wd.\n",
    "\n",
    "### Explanation with little Maths :\n",
    "\n",
    "- we have a cost or error function **E(w)** that we want to minimize. Gradient descent tells us to modify the weights w in the direction of steepest descent in E. This is called backpropogation.\n",
    "   -  $\\begin{equation}w_i \\leftarrow w_i-lr\\frac{\\partial E}{\\partial w_i},\\end{equation}$\n",
    "- where lr is the learning rate, and if it's large you will have a correspondingly large modification of the weights wi (in general it shouldn't be too large, otherwise you'll overshoot the local minimum in your cost function).\n",
    "- In order to effectively limit the number of free parameters in your model so as to avoid over-fitting, it is possible to regularize the cost function remember more parameters means more nonlinearities, more interactions, more curvy bits.Real life is full of curvy bits.\n",
    "- An easy way to do that is by introducing a zero mean Gaussian prior over the weights, which is equivalent to changing the cost function to\n",
    "    - $\\widetilde{E}(\\mathbf{w})=E(\\mathbf{w})+\\frac{wd}{2}\\mathbf{w}^2$\n",
    "- In practice this penalizes large weights and effectively limits the freedom in your model. The regularization parameter wd determines how you trade off the original cost E with the large weights penalization.\n",
    "- Applying gradient descent to this new cost function we obtain: (its just diffrentiation of new loss **$\\widetilde{E}(\\mathbf{w})$**)\n",
    "    - $\\begin{equation}w_i \\leftarrow w_i-lr\\frac{\\widetilde{E}(\\mathbf{w})}{\\partial w_i} \\end{equation}$ Substitute the value of **$\\widetilde{E}(\\mathbf{w})$** from above\n",
    "    - $\\begin{equation}w_i \\leftarrow w_i-lr\\frac{\\partial E}{\\partial w_i}-lr . wd. w_i.\\end{equation}$\n",
    "- The new term **−lr.wd.wi** coming from the regularization causes the weight to decay in proportion to its size.\n",
    "- When it's in this form ($wd . w^2$) where we add the square to the loss function, that's called L2 regularization.\n",
    "- When it's in this form ($wd . w$) where we subtract  times weights from the gradients, that's called **weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://ruder.io/optimizing-gradient-descent/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ba1dbf9748>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://ruder.io/optimizing-gradient-descent/\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://cs231n.github.io/neural-networks-3/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ba1dbf9240>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=\"https://cs231n.github.io/neural-networks-3/\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"http://wiki.fast.ai/index.php/Lesson_3_Notes\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ba1dbf9518>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src=\"http://wiki.fast.ai/index.php/Lesson_3_Notes\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
