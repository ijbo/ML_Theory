{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Weigth Decay ? And why do we care ? The below is what Jermey Howard explains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do a lot of feature engineering and one thing which we do very regularly is to reduce the paramters. \n",
    "- So what is so wrong in this approach ? So why do we care? Why would I want to use more parameters? \n",
    "- Because more parameters means more nonlinearities, more interactions, more curvy bits. \n",
    "- Real life is full of curvy bits. Real life does not look like a straight line. But we don't want them to be more curvy than necessary or more interacting than necessary. Therefore let's use lots of parameters and then penalize complexity.\n",
    "- So one way to penalize complexity is let's sum up the value of your parameters.Now that doesn't quite work because some parameters are positive and some are negative. So what if we sum up the square of the parameters, and that's actually a really good idea.\n",
    "- Let's create a model, and in the loss function we're going to add the sum of the square of the parameters.\n",
    "- Let's actually create a model, and in the loss function we're going to add the sum of the square of the parameters. Now here's a problem with that though. Maybe that number is way too big, and it's so big that the best loss is to set all of the parameters to zero. That would be no good.\n",
    "- So we want to make sure that doesn't happen, so therefore let's not just add the sum of the squares of the parameters to the model but let's multiply that by some number that we choose.That number that we choose is called wd (weigth decay).\n",
    "- loss function and we're going to add to it the sum of the squares of parameters multiplied by some number wd.\n",
    "\n",
    "## Explanation with little Maths :\n",
    "\n",
    "- we have a cost or error function **E(w)** that we want to minimize. Gradient descent tells us to modify the weights w in the direction of steepest descent in E. This is called backpropogation.\n",
    "   -  $\\begin{equation}w_i \\leftarrow w_i-lr\\frac{\\partial E}{\\partial w_i},\\end{equation}$\n",
    "- where lr is the learning rate, and if it's large you will have a correspondingly large modification of the weights wi (in general it shouldn't be too large, otherwise you'll overshoot the local minimum in your cost function).\n",
    "- In order to effectively limit the number of free parameters in your model so as to avoid over-fitting, it is possible to regularize the cost function remember more parameters means more nonlinearities, more interactions, more curvy bits.Real life is full of curvy bits.\n",
    "- An easy way to do that is by introducing a zero mean Gaussian prior over the weights, which is equivalent to changing the cost function to\n",
    "    - $\\widetilde{E}(\\mathbf{w})=E(\\mathbf{w})+\\frac{wd}{2}\\mathbf{w}^2$\n",
    "- In practice this penalizes large weights and effectively limits the freedom in your model. The regularization parameter wd determines how you trade off the original cost E with the large weights penalization.\n",
    "- Applying gradient descent to this new cost function we obtain: (its just diffrentiation of new loss **$\\widetilde{E}(\\mathbf{w})$**)\n",
    "    - $\\begin{equation}w_i \\leftarrow w_i-lr\\frac{\\widetilde{E}(\\mathbf{w})}{\\partial w_i} \\end{equation}$ Substitute the value of **$\\widetilde{E}(\\mathbf{w})$** from above\n",
    "    - $\\begin{equation}w_i \\leftarrow w_i-lr\\frac{\\partial E}{\\partial w_i}-lr . wd. w_i.\\end{equation}$\n",
    "- The new term **âˆ’lr.wd.wi** coming from the regularization causes the weight to decay in proportion to its size.\n",
    "- When it's in this form ($wd . w^2$) where we add the square to the loss function, that's called L2 regularization.\n",
    "- When it's in this form ($wd . w$) where we subtract  times weights from the gradients, that's called **weight decay**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
