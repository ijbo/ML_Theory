{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Regression Analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning :  Regression problem predict real-valued output**.\n",
    "\n",
    "**Supervised learning :  Classification problem predict true/false or 0/1 output**.\n",
    "\n",
    "- Regression analysis is a form of predictive modeling technique which investigates the relationship between a dependent (target) and independent variable (s) (predictor). This technique is used for forecasting, **time series modeling** and finding the causal effect relationship between the variables. For example, relationship between rash driving and number of road accidents by a driver is best studied through regression.\n",
    "\n",
    "- Regression analysis is an important tool for modeling and analyzing data. Here, we fit a curve / line to the data points, in such a manner that the differences between the distances of data points from the curve or line is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we use Regression Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, regression analysis estimates the relationship between two or more variables. Let’s understand this with an easy example:\n",
    "\n",
    "Let’s say, you want to estimate growth in sales of a company based on current economic conditions. You have the recent company data which indicates that the growth in sales is around two and a half times the growth in the economy. Using this insight, we can predict future sales of the company based on current & past information.\n",
    "\n",
    "There are multiple benefits of using regression analysis. They are as follows:\n",
    "\n",
    "1. It indicates the **significant relationships** between dependent variable and independent variable.\n",
    "2. It indicates the **strength of impact** of multiple independent variables on a dependent variable.\n",
    "\n",
    "Regression analysis also allows us to compare the effects of variables measured on different scales, such as the effect of price changes and the number of promotional activities. These benefits help market researchers / data analysts / data scientists to eliminate and evaluate the best set of variables to be used for building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies related to regression analysis\n",
    "\n",
    "### 1. Outliers\n",
    "Suppose there is an observation in the dataset which is having a very high or very low value as compared to the other observations in the data, i.e. it does not belong to the population, such an observation is called an outlier. In simple words, it is extreme value. An outlier is a problem because many times it hampers the results we get.\n",
    "\n",
    "### 2. Multicollinearity\n",
    "When the independent variables are highly correlated to each other then the variables are said to be multicollinear. Many types of regression techniques assumes multicollinearity should not be present in the dataset. It is because it causes problems in ranking variables based on its importance. Or it makes job difficult in selecting the most important independent variable (factor).\n",
    "\n",
    "**Why is Multicollinearity a Potential Problem?**\n",
    "\n",
    "A key goal of regression analysis is to isolate the relationship between each independent variable and the dependent variable. The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables constant. That last portion is crucial for our discussion about multicollinearity.\n",
    "\n",
    "The idea is that you can change the value of one independent variable and not the others. However, when independent variables are correlated, it indicates that changes in one variable are associated with shifts in another variable. The stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend to change in unison.\n",
    "\n",
    "**There are two basic kinds of multicollinearity:**\n",
    "\n",
    "Structural multicollinearity: This type occurs when we create a model term using other terms. In other words, it’s a byproduct of the model that we specify rather than being present in the data itself. For example, if you square term X to model curvature, clearly there is a correlation between X and X2.\n",
    "Data multicollinearity: This type of multicollinearity is present in the data itself rather than being an artifact of our model. Observational experiments are more likely to exhibit this kind of multicollinearity.\n",
    "\n",
    "**Multicollinearity in detail :** https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
    "\n",
    "### 3. Heteroscedasticity\n",
    "When dependent variable’s variability is not equal across values of an independent variable, it is called heteroscedasticity. Example – As one’s income increases, the variability of food consumption will increase. A poorer person will spend a rather constant amount by always eating inexpensive food; a wealthier person may occasionally buy inexpensive food and at other times eat expensive meals. Those with higher incomes display a greater variability of food consumption.\n",
    "\n",
    "**Heteroscedasticity in detail :** https://statisticsbyjim.com/regression/heteroscedasticity-regression/\n",
    "\n",
    "### 4. Underfitting and Overfitting\n",
    "When we use unnecessary explanatory variables it might lead to overfitting. Overfitting means that our algorithm works well on the training set but is unable to perform better on the test sets. It is also known as problem of **high variance**.\n",
    "\n",
    "When our algorithm works so poorly that it is unable to fit even training set well then it is said to underfit the data. It is also known as problem of **high bias**.\n",
    "\n",
    "In the following diagram we can see that fitting a linear regression (straight line in fig 1) would underfit the data i.e. it will lead to large errors even in the training set. Using a polynomial fit in fig 2 is balanced i.e. such a fit can work on the training and test sets well, while in fig 3 the fit will lead to low errors in training set but it will not work well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"11.png\" alt=\"Drawing\" align=\"left\" style=\"width: 700px;\"/> <img src=\"12.png\" alt=\"Drawing\" align=\"left\" style=\"width: 400px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The most commonly used regressions :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression\n",
    "\n",
    "Ordinary Least Squares regression (OLS) is more commonly named linear regression.\n",
    "\n",
    "It is one of the most widely known modeling technique. Linear regression is usually among the first few topics which people pick while learning predictive modeling. In this technique, the dependent variable is continuous, independent variable(s) can be continuous or discrete, and nature of regression line is linear.\n",
    "\n",
    "Linear Regression establishes a relationship between **dependent variable (Y)** and **one or more independent variables (X)** using a best fit straight line (also known as regression line).\n",
    "\n",
    "It is represented by an equation **Y=a+b*X + e**, \n",
    "- a is intercept.\n",
    "- b is slope of the line.\n",
    "- e is error term. \n",
    "This equation can be used to predict the value of target variable based on given predictor variable(s).\n",
    "\n",
    "The difference between **simple linear regression and multiple linear regression** is that : \n",
    "- Multiple linear regression has **(>1)** independent variables.\n",
    "- Simple linear regression has **only 1** independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"13.png\" alt=\"Drawing\" align=\"left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to obtain best fit line (Value of a and b)?\n",
    "This task can be easily accomplished by Least Square Method. It is the most common method used for fitting a regression line. It calculates the best-fit line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line. Because the deviations are first squared, when added, there is no cancelling out between positive and negative values.\n",
    "\n",
    "Residual = Yactual - Ypredicted , in Least square Method or Logistic Regression we minimise the sqaure of the Residual.\n",
    "\n",
    "<img src=\"14.png\" alt=\"Drawing\" align=\"left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function = $\\frac{1}{2m}\\sum_{i=0}^m (Yactual - Ypredict)^2$\n",
    "\n",
    "**Goal** is to minimise the Cost function in linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- To minimise the cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stepwise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ElasticNet Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
