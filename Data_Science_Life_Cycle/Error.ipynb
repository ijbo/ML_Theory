{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression performance\n",
    "- R2 and adjusted R2 (aka explained variance)\n",
    "\n",
    "- Mean squared error (MSE), or root mean squared error (RMSE)\n",
    "\n",
    "- Mean error, or mean absolute error\n",
    "\n",
    "- Median error, or median absolute error\n",
    "\n",
    "# Classification performance\n",
    "- Confusion matrix\n",
    "\n",
    "- Precision\n",
    "\n",
    "- Recall (aka sensitivity)\n",
    "\n",
    "- Specificity\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- Lift : Lift is nothing but the ratio of Confidence to Expected Confidence\n",
    "\n",
    "- Area under the ROC curve (AUC)\n",
    "\n",
    "- F-score\n",
    "\n",
    "- Log-loss\n",
    "\n",
    "- Average precision\n",
    "\n",
    "- Precision/recall break-even point\n",
    "\n",
    "- Root mean squared error (RMSE)\n",
    "\n",
    "- Mean cross entropy\n",
    "\n",
    "- Probability calibration\n",
    "\n",
    "# Bias variance tradeoff and model complexity\n",
    "- Validation curve\n",
    "\n",
    "- Learning curve\n",
    "\n",
    "- Residual sum of squares\n",
    "\n",
    "- Goodness-of-fit metrics\n",
    "\n",
    "# Model validation and selection\n",
    "- Mallow’s Cp\n",
    "\n",
    "- Akaike information criterion (AIC)\n",
    "\n",
    "- Bayesian information criterion (BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation \n",
    "\n",
    "[![IMAGE ALT TEXT](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=fSytzGwwBVw&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=2 \"Cross Validation\")\n",
    "\n",
    "- Cross validation allows us to compare different machine learning methods tells us how well will it work.\n",
    "- We need to estimate the parameters for a machine learning method , training the algorithm\n",
    "- Evaluate how well a machine learning method is going to work , testing the algorithm.\n",
    "- 75% data for training and 25% in testing. we divided into 4 block called four-fold cross validation.\n",
    "- In Cross Validation total data is divided in N sets , 1 set is used for testing and other set for training . Then in the next different 1 set is used for test and different N-1 set is used to train.\n",
    "- It keeps track of how well the method did with the data.\n",
    "- In an extreme case we call each individual patient(or sample) as a block. This is called \"Leave one out Cross validation\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "[![IMAGE ALT TEXT](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=Kdsp6soqA7o&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=3 \"Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Cheat Pain</td><td>Good Blood circ</td><td>Blocked Arteries</td><td>Weight</td><td>Heart Disease</td></tr>\n",
       "<tr><td>No        </td><td>No             </td><td>No              </td><td>125   </td><td>No           </td></tr>\n",
       "<tr><td>Yes       </td><td>Yes            </td><td>Yes             </td><td>180   </td><td>Yes          </td></tr>\n",
       "<tr><td>Yes       </td><td>Yes            </td><td>No              </td><td>210   </td><td>No           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"Cheat Pain \",\"Good Blood circ\",\"Blocked Arteries\",\"Weight\",\"Heart Disease\"],\n",
    "         [\"No\",\"No\",\"No\",125,\"No\"],\n",
    "         [\"Yes\",\"Yes\",\"Yes\",180,\"Yes\"],\n",
    "         [\"Yes\",\"Yes\",\"No\",210,\"No\"]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To create a confusion matrix.\n",
    "- We start by dividing the data in training and test sets , called as cross validation.\n",
    "- Train all the methods in the training data .\n",
    "- Test each method in the testing set.\n",
    "- Summaries how each method performed on Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Predicted                  </td><td><---- Actual ---></td><td>                           </td></tr>\n",
       "<tr><td>                           </td><td>Has Heart Disease</td><td>Does not have Heart Disease</td></tr>\n",
       "<tr><td>Has Heart Disease          </td><td>True Positive    </td><td>False Positive             </td></tr>\n",
       "<tr><td>Does not have Heart Disease</td><td>False Negative   </td><td>True Negative              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [ [\"Predicted \",\"<---- Actual --->\"],\n",
    "         [\" \",\"Has Heart Disease\",\"Does not have Heart Disease\"],\n",
    "         [\"Has Heart Disease\",\"True Positive\",\"False Positive \"],\n",
    "         [\"Does not have Heart Disease\",\"False Negative \",\" True Negative\"]\n",
    "        ]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can apply Random forest and K-Nearest Neighbors and get the confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type of Error in Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Measurement in Classification Problems\n",
    "![title](1.png) \n",
    "\n",
    "\n",
    "Classification problems are usually binary identification determining if an observation is, or is not, a certain condition. Out of any classification model there are four types of results.\n",
    "\n",
    "- True Positive(TP) -A true positive test result is one that detects the condition when the condition is present.\n",
    "\n",
    "- False Positive (FP)-Also know as a Type I error, a false positive test result is one that detects the condition when the condition is absent.\n",
    "\n",
    "- False Negative (FN)-Also know as a Type II error, a false negative test result is one that does not detect the condition when the condition is present.\n",
    "\n",
    "- True Negative (TN)- A true negative test result is one that does not detect the condition when the condition is absent.\n",
    "\n",
    "Error is calculated of different ratios and formulas based on these four states. It is easy to see that depending on the cost of a Type I or a Type II the way the error is measured might be adjusted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Error\n",
    "To understand how a model is performing, there are a variety of ways to measure the interplay of the types of conditions. \n",
    "A Confusion Matrix (yes, that is really what it is called) is used to present multiple types of error measurements so a data scientist can determine if the model is performing well or not. \n",
    "Below we will cover the following types of error measurements:\n",
    "\n",
    "- Specificity or True Negative Rate (TNR)\n",
    "- Precision, Positive Predictive Value (PPV)\n",
    "- Recall, Sensitivity, Hit Rate or True Positive Rate (TPR)\n",
    "- F Measure (F1,F0.5,F2)\n",
    "- Matthew’s Correlation Coefficient (MCC)\n",
    "- ROC Area (ROC AUC)\n",
    "- Fallout,False Positive Rate (FPR)\n",
    "- R², Coefficient of Determination (r²)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity or True Negative Rate (TNR)\n",
    "- TNR (ranges from 0 to 1, higher is better) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition).\n",
    "- ### TNR = TN/(TN+FP)\n",
    "\n",
    "<img src=\"2.png\" alt=\"Drawing\" align=\"left\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Positive Predictive Value (PPV)\n",
    "\n",
    "- PPV (ranges from 0 to 1, higher is better) is the ratio of true positives over all true and false positives:\n",
    "- PPV = TP/(TP+FP)\n",
    "- High precision means that an algorithm returned substantially more relevant results than irrelevant ones, or in other word the more likely everything it returns is right, but it does not mean it may get all the right results that are out there.\n",
    "- Likewise this can be done with Negative Predictive Value (NPV) with positive flipped to negative and calculated to determine precision in the negative predictions. \n",
    "- The complement of the NPV is the false omission rate (FOR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall, Sensitivity, Hit Rate or True Positive Rate (TPR)\n",
    "\n",
    "- TPR (ranges from 0 to 1, higher is better) is the ratio of true positives over the sum of true positives and false negatives:\n",
    "- TPR = TP / (TP+FN)\n",
    "- High recall means that an algorithm returned most of the relevant results, but it may have a bunch of false returns as well like a drag net that will certainly grab the fish you want but also catch a bunch you don’t want.\n",
    "\n",
    "<img src=\"3.png\" alt=\"Drawing\" align=\"left\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F Measure\n",
    "<img src=\"4.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F Measure (ranges from 0–1) is a ratio that describes the balance between Precision (PPV) and Recall (TPR). Using the harmonic mean it can describe how heavily a model may be leaning one way or another.\n",
    "\n",
    "- F = (PPV*TPR)/(PPV+TPR)\n",
    "or\n",
    "- F = 2TP/(2TP+FP+FN)\n",
    "\n",
    "- The most common is called F1, while two other commonly used F measures are the F2 measure, which weights recall higher than precision, and the F0.5 measure, which puts more emphasis on precision than recall.\n",
    "\n",
    "\n",
    "- F-score is a combined measure of precision and recall.    \n",
    "- An F1 score gives equal weight to precision and recall.   \n",
    "- The beta symbol, ß ,  is used in mathematics to indicate when a variable can be entered.   \n",
    "- The term F2 score will be used when twice the weight is given to recall as opposed to precision.   \n",
    "- When giving twice as much weight to precision, an F 0.5 score is used.   \n",
    "\n",
    "A version of the equation, which allows different weights to be assigned to precision or recall would be expressed this way:\n",
    "\n",
    "\n",
    "$ Fß = (1+ß²) * \\frac {(Precision  * Recall)} {(ß² * Precision) + Recall}$\n",
    "\n",
    "$ F1 Score =  2 * \\frac {(Precision * Recall)} {(Precision + Recall)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matthew’s Correlation Coefficient (MCC)\n",
    "\n",
    "- The MCC (ranges from -1 to 1) was introduced in 1975 by biochemist Brian W. Matthews. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications. A coefficient of +1 represents a perfect prediction, 0 is equal to no better than random prediction and −1 indicates total disagreement between prediction and observation. This is often represented as a correlation heatmap like it is here, and allows for quick observations about which features are useful (further in depth reading here about all the types of coorelation).\n",
    "- MCC = (TP*TN — FP*FN)/Sqr(FT+FP)(TN+FP)(TN+FN)\n",
    "- Values of 0.05+ can be useful as features for a model and negative correlation can, in certain situations, be helpful too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC - Receiver Operating Characteristic\n",
    "\n",
    "- ROC Graph summarizes all the confusion matrices that each threshold produces.\n",
    "- ROC graph is True Positive Rate (Sensitivity) vs False Positive Rate (1-Specificity)\n",
    "- Without looking through all the confusion Matrices , it is easy to identify the better threshold.\n",
    "- Depending on how many false positive you are willing to accept the threshold can be decided.\n",
    "- Sensitivity, True Positive Rate TPR = TP / (TP+FN)\n",
    "- Specificity, True Negative Rate TNR = TN / (TN+FP)\n",
    "- Fallout,False Positive Rate = 1 - Specificity (the higher the value the worse as that means there are more proportionally more false negatives identified)\n",
    "- The point left to the gren threshold is better than the point on the green line.\n",
    "- Green Line means True Positives = False Positive Rate\n",
    "\n",
    "<img src=\"6.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC - Area Under the Curve \n",
    "\n",
    "- AUC makes it easy to compare one ROC curve to another .\n",
    "- If the Red ROC curve represent logistic regression and the Blue ROC curve represent Random Forest, you would use the Logistic regression.\n",
    "- Flase Positive Rate can be replace by precision, incase there are lot of Positive sample. Like in the study of rare disease, in this case there will be many more people without the disease than with the disease\n",
    "- Precision = True Positives/(True Positives + False Positives)\n",
    "- Precision is the proportion of the positive results that are correctly classified.\n",
    "\n",
    "\n",
    "<img src=\"5.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ROC and AUC Video Explanation\n",
    "\n",
    "[![IMAGE ALT TEXT](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=xugjARegisk&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=6 \"ROC and AUC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy (ACC)\n",
    "\n",
    "- Accuracy (ranges from 0 to 1, higher is better) is simply a ratio of correctly predicted observation to the total observations.\n",
    "- ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "- Instinctively one would assume accuracy is a great measure but it actually tells you very little about false positives and negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R², Coefficient of Determination\n",
    "\n",
    "- Create a X, Y plot , Find a line which minimises the sqaured distance between the point.\n",
    "- Sqaured Error is the distance between the point and the line .\n",
    "- Total Sqaured error between the points and the line e1 = y1 -(mx1 + b ) , Square Error from the line  = (y1 - (mx1+b))^2 + .... (yn - (mxn+b))^2\n",
    "- How good is the line fitting the data points ? How much (what % ) of the variation in y is described by the variation in x ?\n",
    "- Total variation in Y (Total sqaured distance) or Squared Error from the Mean  = (y1 - mean(y))^2 + (y2 - mean(y))^2 + ... (yn - mean(y))^2\n",
    "- Variance of Y (Average sqaured distance) = {(y1 - mean(y))^2 + (y2 - mean(y))^2 + ... (yn - mean(y))^2} / n\n",
    "- How much of the total variation is not described by the  of the regression LINE ? Total Sqaured error\n",
    "- What % of variation is not described by the variation in X or by the line ? Square Error/Squared Error from the Mean\n",
    "- How much (what % ) of the variation in y is described by the variation in x ? 1-(Square Error from the Line /Squared Error from the Mean) \n",
    "- Coefficient of Determination or R² = 1-(Square Error from the Line /Squared Error from the Mean) \n",
    "- If the Square Error from the Line is really small , which means the line is a very good fit, R² will be close to 1.\n",
    "- If the Square Error from the Line is really large , which means the line is a very bad  fit, R² will be close to 0. That means very little of the total variation in Y is described by the variation in x.\n",
    "\n",
    "<img src=\"7.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions and Error Methods\n",
    "- Unlike the classification problems above, regressions don’t produce binary absolute values but rather a numeric range. Ideally algorithms should be stable, although what that means is largely dependent on the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Performance Measure of an Algorithm\n",
    "   ### Root Mean Square Error (RMSE) : \n",
    "    A typical performance measure for regression problems is the Root Mean Square Error (RMSE). \n",
    "    It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RMSE(X,h)= \\sqrt{\\frac{1}{m}\\Sigma_{i=1}^{m}{(h(x(i)-y(i))^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- m is the number of instances in the dataset you are measuring the RMSE on. For example, if you are evaluating the RMSE on a validation set of 2,000 districts, then m = 2,000.\n",
    "- x(i) is a vector of all the feature values (excluding the label) of the ith instance in the dataset, and y(i) is its label (the desired output value for that instance).\n",
    "- h is your system’s prediction function, also called a hypothesis.\n",
    "- RMSE(X,h) is the cost function measured on the set of examples using your hypothesis h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (also called the Average Absolute Deviation): \n",
    "\n",
    "$MAE(X,h)= {\\frac{1}{m}\\Sigma_{i=1}^{m}|{(h(x(i)-y(i))}|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values.\n",
    "-  Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: it is the notion of distance you are familiar with. It is also called the ℓ2 norm, noted ∥ · ∥2 (or just ∥ · ∥).\n",
    "- Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm, noted ∥ · ∥1. It is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n",
    "- The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample = 10 \n",
    "- MAE = sum(|Error|)/Sample\n",
    "- RMSE=SQRT(sum(ERROR^2)/Sample)\n",
    "- Var = (sum(Error – Mean(Error)))^2/Sample -1\n",
    "- Mean = Sum(Error)/Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error MSE :\n",
    "- We can instead take the mean of the squared error values, which is called the mean squared error or MSE for short. The MSE makes the gap between the predicted and actual values more clear\n",
    "\n",
    "$MSE=\\frac{1}{n}\\Sigma_{k=1}^{n} (actual_1 - predicted_1)^2+ ......+(actual_n - predicted_n)^2$\n",
    "\n",
    "$RMSE=\\sqrt{MSE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences: \n",
    "- Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors.\n",
    "- This means the RMSE should be more useful when large errors are particularly undesirable. \n",
    "- The three tables below show examples where MAE is steady and RMSE increases as the variance associated with the frequency distribution of error magnitudes also increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e = $(actual_1 - predicted_1)+ ......+(actual_n - predicted_n)$\n",
    "#### Mean squared error (MSE) = $\\frac{1}{n}\\sum_{t=1}^{n}e_t^2$\n",
    "#### Root mean squared error (RMSE) = $\\sqrt{\\frac{1}{n}\\sum_{t=1}^{n}e_t^2}$\n",
    "#### Mean absolute error (MAE)  =  $\\frac{1}{n}\\sum_{t=1}^{n}|e_t|$\n",
    "#### Mean absolute percentage error (MAPE) = $\\frac{100\\%}{n}\\sum_{t=1}^{n}\\left |\\frac{e_t}{y_t}\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lift : Lift is nothing but the ratio of Confidence to Expected Confidence\n",
    "\n",
    "- Imagine you are running a direct mail campaign where you mail customers an offer in the hopes they respond. Historical data shows that when you mail your customer base completely at random about 8% of them respond to the mailing (i.e. they come in and shop with the offer). So, if you mail 1,000 customers you can expect 80 responders.\n",
    "\n",
    "- Now, you decide to fit a logistic regression model to your historical data to find patterns that are predictive of whether a customer is likely to respond to a mailing. Using the logistic regression model each customer is assigned a probability of responding and you can assess the accuracy because you know whether they actually responded. Once each customer is assigned their probability, you rank them from highest to lowest scoring customer. Then you could generate some \"lift\" graphics like these:\n",
    "\n",
    "<img src=\"8.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ignore the top chart for now. The bottom chart is saying that after we sort the customers based on their probability of responding (high to low), and then break them up into ten equal bins, the response rate in bin #1 (the top 10% of customers) is 29% vs 8% of random customers, for a lift of 29/8 = 3.63. By the time we get to scored customers in the 4th bin, we have captured so many the previous three that the response rate is lower than what we would expect mailing people at random.\n",
    "\n",
    "- Looking at the top chart now, what this says is that if we use the probability scores on customers we can get 60% of the total responders we'd get mailing randomly by only mailing the top 30% of scored customers. That is, using the model we can get 60% of the expected profit for 30% of the mail cost by only mailing the top 30% of scored customers, and this is what lift really refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-loss or Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logarithmic loss (related to cross-entropy) measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of our machine learning models is to minimize this value. A perfect model would have a log loss of 0. Log loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high log loss. There is a more detailed explanation of the justifications and math behind log loss here.\n",
    "\n",
    "- Log Loss is the most important classification metric based on probabilities.\n",
    "\n",
    "- It's hard to interpret raw log-loss values, but log-loss is still a good metric for comparing models. For any given problem, a lower log-loss value means better predictions.\n",
    "\n",
    "- Log Loss is a slight twist on something called the Likelihood Function. In fact, Log Loss is -1 * the log of the likelihood function. So, we will start by understanding the likelihood function.\n",
    "\n",
    "- The likelihood function answers the question \"How likely did the model think the actually observed set of outcomes was.\" If that sounds confusing, an example should help\n",
    "\n",
    "#### Example\n",
    "- A model predicts probabilities of [0.8, 0.4, 0.1] for three houses. The first two houses were sold, and the last one was not sold. So the actual outcomes could be represented numeically as [1, 1, 0].\n",
    "\n",
    "- Let's step through these predictions one at a time to iteratively calculate the likelihood function.\n",
    "\n",
    "- The first house sold, and the model said that was 80% likely. So, the likelihood function after looking at one prediction is 0.8.\n",
    "\n",
    "- The second house sold, and the model said that was 40% likely. There is a rule of probability that the probability of multiple independent events is the product of their individual probabilities. So, we get the combined likelihood from the first two predictions by multiplying their associated probabilities. That is 0.8 * 0.4, which happens to be 0.32.\n",
    "\n",
    "- Now we get to our third prediction. That home did not sell. The model said it was 10% likely to sell. That means it was 90% likely to not sell. So, the observed outcome of not selling was 90% likely according to the model. So, we multiply the previous result of 0.32 by 0.9.\n",
    "\n",
    "- We could step through all of our predictions. Each time we'd find the probability associated with the outcome that actually occurred, and we'd multiply that by the previous result. That's the likelihood.\n",
    "\n",
    "### From Likelihood to Log Loss\n",
    "- Each prediction is between 0 and 1. If you multiply enough numbers in this range, the result gets so small that computers can't keep track of it. So, as a clever computational trick, we instead keep track of the log of the Likelihood. This is in a range that's easy to keep track of. We multiply this by negative 1 to maintain a common convention that lower loss scores are better.\n",
    "\n",
    "[Log-loss Details by Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html \"Log loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss Defination\n",
    "\n",
    "- Cost = -(Yactual) Ln (Ypredicted) - (1-Yactual) Ln (1-Ypredicted)\n",
    "\n",
    "- Yactual = 0 -> Cost = -Ln (1-Ypredicted)\n",
    "\n",
    "- Yactual = 1 -> Cost = -Ln (Ypredicted)\n",
    "\n",
    "\n",
    "### Graphs :\n",
    "\n",
    "- Yactual = 1 , Ypred varies from 0->1\n",
    "- cost = -ln(0->1)\n",
    "- Xaxis = Ypred: 0->1\n",
    "- Yaxis = -ln(0->1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"9.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Yactual = 0 , Ypred varies from 0->1\n",
    "- cost = -ln(1 - 0->1)\n",
    "- Xaxis = Ypred: 0->1\n",
    "- Yaxis = -ln(1 - 0->1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"10.png\" alt=\"Drawing\" align=\"Left\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEY NOTE : Cost function Penalises the confident and wrong prediction more than the it Rewards the confident and right predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions in Neural Networks : https://isaacchanghau.github.io/post/loss_functions/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
